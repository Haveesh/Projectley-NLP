{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "import random\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm_notebook, trange\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "\n",
    "from pytorch_transformers import (WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer)\n",
    "\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "from utils import (convert_examples_to_features,\n",
    "                        output_modes, processors)\n",
    "from utils_seedly import processors as seedly_processor\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for training\n",
    "Configuration for such as: director for data, log and outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'data_dir': 'data/',\n",
    "    'model_type':  'bert',\n",
    "    'model_name': 'bert-base-cased',\n",
    "    'task_name': 'binary',\n",
    "    'output_dir': 'outputs_yelp_logged_LSTM_report/',\n",
    "    'cache_dir': 'cache/',\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'fp16': False,\n",
    "    'fp16_opt_level': 'O1',\n",
    "    'max_seq_length': 128,\n",
    "    'output_mode': 'classification',\n",
    "    'train_batch_size': 64,\n",
    "    'eval_batch_size': 64,\n",
    "\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'num_train_epochs': 1,\n",
    "    'weight_decay': 0,\n",
    "    'learning_rate': 4e-5,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'warmup_ratio': 0.06,\n",
    "    'warmup_steps': 0,\n",
    "    'max_grad_norm': 1.0,\n",
    "\n",
    "    'logging_steps': 500,\n",
    "    'evaluate_during_training': False,\n",
    "    'save_steps': 2000,\n",
    "    'eval_all_checkpoints': True,\n",
    "\n",
    "    'overwrite_output_dir': False,\n",
    "    'reprocess_input_data': False,\n",
    "    'notes': 'Using Yelp Reviews dataset'\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment this if you want to train for the first time\n",
    "# if os.path.exists(args['output_dir']) and os.listdir(args['output_dir']) and args['do_train'] and not args['overwrite_output_dir']:\n",
    "#     raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args['output_dir']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_class, model_class, tokenizer_class = (BertConfig, BertForSequenceClassification, BertTokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer for BERT\n",
    "We use same tokenizer with BERT to compare more accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/chungsoo002/.cache/torch/pytorch_transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6\n",
      "INFO:pytorch_transformers.modeling_utils:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"binary\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 5,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/chungsoo002/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    }
   ],
   "source": [
    "config = config_class.from_pretrained(args['model_name'], num_labels=5, finetuning_task=args['task_name'])\n",
    "tokenizer = tokenizer_class.from_pretrained(args['model_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define LSTM Model \n",
    "On top of BERT tokenizer, we stack 5 layers of LSTM with hidden dimension of 768."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "class LSTM_yelp(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LSTM_yelp, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(28996, 768)\n",
    "        self.rnn = torch.nn.LSTM(768, 768, num_layers=5, dropout=0.3, batch_first=True)\n",
    "        self.output = torch.nn.Linear(768, 5)\n",
    "\n",
    "        \n",
    "    def forward(self, input_ids, labels):\n",
    "        emb = self.embedding(input_ids)\n",
    "        h, _ = self.rnn(emb)\n",
    "        logit = self.output(h[:, -1, :])\n",
    "        loss = criterion(logit, labels)\n",
    "        \n",
    "        return loss, logit, []\n",
    "    \n",
    "    def save_pretrained(self, save_directory):\n",
    "        \"\"\" Save a model and its configuration file to a directory, so that it\n",
    "            can be re-loaded using the `:func:`~transformers.PreTrainedModel.from_pretrained`` class method.\n",
    "        \"\"\"\n",
    "        assert os.path.isdir(save_directory), \"Saving path should be a directory where the model and configuration can be saved\"\n",
    "\n",
    "        # Only save the model it-self if we are using distributed training\n",
    "        model_to_save = self.module if hasattr(self, 'module') else self\n",
    "\n",
    "        # Save configuration file\n",
    "#         model_to_save.config.save_pretrained(save_directory)\n",
    "\n",
    "        # If we save using the predefined names, we can load using `from_pretrained`\n",
    "        output_model_file = os.path.join(save_directory, WEIGHTS_NAME)\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        logger.info(\"Model weights saved in {}\".format(output_model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment this if you want to train for the first time\n",
    "# model = LSTM_yelp()\n",
    "# model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "Here we define functions to efficiently load data and then feed into the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = args['task_name']\n",
    "\n",
    "if task in processors.keys() and task in output_modes.keys():\n",
    "    processor = processors[task]()\n",
    "    label_list = processor.get_labels()\n",
    "    num_labels = len(label_list)\n",
    "else:\n",
    "    raise KeyError(f'{task} not found in processors or in output_modes. Please check utils.py.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class yelpDataset(Dataset):\n",
    "  def __init__(self, examples, label_list, args, tokenizer, output_mode):\n",
    "    self.examples=examples\n",
    "    self.label_list = label_list\n",
    "    self.args = args\n",
    "    self.tokenizer = tokenizer\n",
    "    self.output_mode = output_mode\n",
    "      \n",
    "  def __len__(self):\n",
    "    # return size of dataset\n",
    "    return len(self.examples)\n",
    "      \n",
    "  def __getitem__(self, idx):\n",
    "    features = convert_examples_to_features(self.examples[idx], self.label_list, self.args['max_seq_length'], self.tokenizer, self.output_mode,\n",
    "                cls_token_at_end=bool(self.args['model_type'] in ['xlnet']),            \n",
    "                cls_token=self.tokenizer.cls_token,\n",
    "                cls_token_segment_id=2 if self.args['model_type'] in ['xlnet'] else 0,\n",
    "                sep_token=self.tokenizer.sep_token,\n",
    "                sep_token_extra=bool(self.args['model_type'] in ['roberta']),           \n",
    "                pad_on_left=bool(self.args['model_type'] in ['xlnet']),                 \n",
    "                pad_token=self.tokenizer.convert_tokens_to_ids([self.tokenizer.pad_token])[0],\n",
    "                pad_token_segment_id=4 if self.args['model_type'] in ['xlnet'] else 0)\n",
    "    \n",
    "    all_input_ids = torch.tensor(features.input_ids, dtype=torch.long)\n",
    "    all_input_mask = torch.tensor(features.input_mask, dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor(features.segment_ids, dtype=torch.long)\n",
    "    if self.output_mode == \"classification\":\n",
    "        all_label_ids = torch.tensor(features.label_id, dtype=torch.long)\n",
    "    elif self.output_mode == \"regression\":\n",
    "        all_label_ids = torch.tensor(features.label_id, dtype=torch.float)\n",
    "\n",
    "    dataset = (all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(task, tokenizer, mode):\n",
    "    processor = processors[task]()\n",
    "    output_mode = args['output_mode']\n",
    "    \n",
    "    \n",
    "    label_list = processor.get_labels()\n",
    "    if mode == 'train':\n",
    "        examples = processor.get_train_examples(args['data_dir'])\n",
    "    elif mode == 'eval':\n",
    "        examples = processor.get_dev_examples(args['data_dir'])\n",
    "    elif mode == 'test':\n",
    "        examples = processor.get_test_examples(args['data_dir'])\n",
    "    \n",
    "        \n",
    "    if __name__ == \"__main__\":\n",
    "        dataset=yelpDataset(examples, label_list, args, tokenizer, output_mode)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to compute evaluation metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, matthews_corrcoef, confusion_matrix\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def get_mismatched(labels, preds):\n",
    "    mismatched = labels != preds\n",
    "    examples = processor.get_dev_examples(args['data_dir'])\n",
    "    wrong = [i for (i, v) in zip(examples, mismatched) if v]\n",
    "    \n",
    "    return wrong\n",
    "\n",
    "def get_eval_report(labels, preds):\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    cf = confusion_matrix(labels, preds, labels=[0,1,2,3,4])\n",
    "    return {\n",
    "        \"mcc\": mcc,\n",
    "        \"cf\": cf\n",
    "    }, get_mismatched(labels, preds)\n",
    "\n",
    "def compute_metrics(task_name, preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "    return get_eval_report(labels, preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Eval, Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment this if you want to train for the first time\n",
    "# def train(dataset, model, tokenizer):\n",
    "#     EVAL_TASK = args['task_name']\n",
    "#     tb_writer = SummaryWriter('./log_yelp_train_LSTM_report')\n",
    "#     train_dataset, eval_dataset, test_dataset = dataset\n",
    "    \n",
    "#     train_sampler = RandomSampler(train_dataset)\n",
    "#     train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args['train_batch_size'])\n",
    "#     eval_sampler = SequentialSampler(eval_dataset)\n",
    "#     eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n",
    "#     test_sampler = SequentialSampler(test_dataset)\n",
    "#     test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args['eval_batch_size'])\n",
    "    \n",
    "#     t_total = len(train_dataloader) // args['gradient_accumulation_steps'] * args['num_train_epochs']\n",
    "    \n",
    "#     no_decay = ['bias', 'LayerNorm.weight']\n",
    "#     optimizer_grouped_parameters = [\n",
    "#         {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args['weight_decay']},\n",
    "#         {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "#         ]\n",
    "    \n",
    "#     warmup_steps = math.ceil(t_total * args['warmup_ratio'])\n",
    "#     args['warmup_steps'] = warmup_steps if args['warmup_steps'] == 0 else args['warmup_steps']\n",
    "    \n",
    "#     optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'])\n",
    "#     scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args['warmup_steps'], t_total=t_total)\n",
    "    \n",
    "#     if args['fp16']:\n",
    "#         try:\n",
    "#             from apex import amp\n",
    "#         except ImportError:\n",
    "#             raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "#         model, optimizer = amp.initialize(model, optimizer, opt_level=args['fp16_opt_level'])\n",
    "        \n",
    "#     logger.info(\"***** Running training *****\")\n",
    "#     logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "#     logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n",
    "#     logger.info(\"  Total train batch size  = %d\", args['train_batch_size'])\n",
    "#     logger.info(\"  Gradient Accumulation steps = %d\", args['gradient_accumulation_steps'])\n",
    "#     logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "#     global_step = 0\n",
    "    \n",
    "#     model.zero_grad()\n",
    "#     train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")\n",
    "#     epoch = 0\n",
    "#     for _ in train_iterator:\n",
    "#         epoch += 1\n",
    "#         epoch_iterator = tqdm_notebook(train_dataloader, desc=\"Iteration\")\n",
    "#         tr_loss, logging_loss = 0.0, 0.0\n",
    "#         tr_acc, log_acc = 0.0, 0.0\n",
    "#         nb_train_steps = 0\n",
    "#         for step, batch in enumerate(epoch_iterator):\n",
    "#             model.train()\n",
    "#             batch = tuple(t.to(device) for t in batch)\n",
    "#             inputs = {'input_ids':      batch[0],\n",
    "#                       'attention_mask': batch[1],\n",
    "#                       'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
    "#                       'labels':         batch[3]}\n",
    "#             outputs = model(batch[0], batch[3])\n",
    "#             loss, logits = outputs[0:2]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "#             print(\"\\r%f\" % loss, end='')\n",
    "#             nb_train_steps += 1\n",
    "#             preds = logits.detach().cpu().numpy()\n",
    "#             out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "\n",
    "#             if args['output_mode'] == \"classification\":\n",
    "#                 preds = np.argmax(preds, axis=1)\n",
    "#             elif args['output_mode'] == \"regression\":\n",
    "#                 preds = np.squeeze(preds)\n",
    "#             result, wrong = compute_metrics(EVAL_TASK, preds, out_label_ids)\n",
    "#             c = result['cf']\n",
    "#             acc=(c[0,0] + c[1,1] + c[2,2] + c[3,3] + c[4,4])/np.sum(np.sum(c))\n",
    "\n",
    "\n",
    "#             if args['gradient_accumulation_steps'] > 1:\n",
    "#                 loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "#             if args['fp16']:\n",
    "#                 with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "#                     scaled_loss.backward()\n",
    "#                 torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args['max_grad_norm'])\n",
    "\n",
    "#             else:\n",
    "#                 loss.backward()\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n",
    "\n",
    "#             tr_loss += loss.item()\n",
    "#             tr_acc = (tr_acc * (nb_train_steps-1) + acc)/(nb_train_steps) \n",
    "#             if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "#                 optimizer.step()\n",
    "#                 scheduler.step()  # Update learning rate schedule\n",
    "#                 model.zero_grad()\n",
    "#                 global_step += 1\n",
    "\n",
    "#                 if args['logging_steps'] > 0 and global_step % args['logging_steps'] == 0:\n",
    "#                     # Log metrics\n",
    "#                     tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "#                     tb_writer.add_scalar('train_loss', (tr_loss - logging_loss)/args['logging_steps'], global_step)\n",
    "#                     logging_loss = tr_loss\n",
    "#                     tb_writer.add_scalar('train_acc', tr_acc, global_step)\n",
    "                    \n",
    "#                     # Eval!\n",
    "#                     prefix = global_step\n",
    "#                     logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "#                     logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "#                     logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "#                     eval_loss = 0.0\n",
    "#                     nb_eval_steps = 0\n",
    "#                     preds = None\n",
    "#                     out_label_ids = None\n",
    "#                     eval_gen=tqdm_notebook(eval_dataloader, desc=\"Evaluating\")\n",
    "#                     for (eval_step,batch) in enumerate(eval_gen):      \n",
    "#                         if eval_step%100==0:\n",
    "#                             model.eval()\n",
    "#                             batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "#                             with torch.no_grad():\n",
    "#                                 inputs = {'input_ids':      batch[0],\n",
    "#                                           'attention_mask': batch[1],\n",
    "#                                           'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
    "#                                           'labels':         batch[3]}\n",
    "#                                 outputs = model(batch[0], batch[3])\n",
    "#                                 tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "#                                 eval_loss += tmp_eval_loss.mean().item()\n",
    "#                             nb_eval_steps += 1\n",
    "#                             if preds is None:\n",
    "#                                 preds = logits.detach().cpu().numpy()\n",
    "#                                 out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "#                             else:\n",
    "#                                 preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "#                                 out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "#                     eval_loss = eval_loss / nb_eval_steps\n",
    "#                     if args['output_mode'] == \"classification\":\n",
    "#                         preds = np.argmax(preds, axis=1)\n",
    "#                     elif args['output_mode'] == \"regression\":\n",
    "#                         preds = np.squeeze(preds)\n",
    "#                     result, wrong = compute_metrics(EVAL_TASK, preds, out_label_ids)\n",
    "#                     c = result['cf']\n",
    "#                     eval_acc=(c[0,0] + c[1,1] + c[2,2] + c[3,3] + c[4,4])/np.sum(np.sum(c))\n",
    "#                     tb_writer.add_scalar('eval_loss', eval_loss, global_step)\n",
    "#                     tb_writer.add_scalar('eval_acc', eval_acc, global_step)\n",
    "                    \n",
    "#                     # Test!\n",
    "#                     logger.info(\"***** Running test {} *****\".format(prefix))\n",
    "#                     logger.info(\"  Num examples = %d\", len(test_dataset))\n",
    "#                     logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "#                     test_loss = 0.0\n",
    "#                     nb_test_steps = 0\n",
    "#                     preds = None\n",
    "#                     out_label_ids = None\n",
    "#                     test_gen = tqdm_notebook(test_dataloader, desc=\"Testing\")\n",
    "#                     for (test_step,batch) in enumerate(test_gen):\n",
    "#                         if test_step%100==0:\n",
    "#                             model.eval()\n",
    "#                             batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "#                             with torch.no_grad():\n",
    "#                                 inputs = {'input_ids':      batch[0],\n",
    "#                                           'attention_mask': batch[1],\n",
    "#                                           'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
    "#                                           'labels':         batch[3]}\n",
    "#                                 outputs = model(batch[0], batch[3])\n",
    "#                                 tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "#                                 test_loss += tmp_eval_loss.mean().item()\n",
    "#                             nb_test_steps += 1\n",
    "#                             if preds is None:\n",
    "#                                 preds = logits.detach().cpu().numpy()\n",
    "#                                 out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "#                             else:\n",
    "#                                 preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "#                                 out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "#                     test_loss = test_loss / nb_test_steps\n",
    "#                     if args['output_mode'] == \"classification\":\n",
    "#                         preds = np.argmax(preds, axis=1)\n",
    "#                     elif args['output_mode'] == \"regression\":\n",
    "#                         preds = np.squeeze(preds)\n",
    "#                     result, wrong = compute_metrics(EVAL_TASK, preds, out_label_ids)\n",
    "#                     c = result['cf']\n",
    "#                     test_acc=(c[0,0] + c[1,1] + c[2,2] + c[3,3] + c[4,4])/np.sum(np.sum(c))\n",
    "#                     tb_writer.add_scalar('test_loss', test_loss, global_step)\n",
    "#                     tb_writer.add_scalar('test_acc', test_acc, global_step)\n",
    "                    \n",
    "\n",
    "#                 if args['save_steps'] > 0 and global_step % args['save_steps'] == 0:\n",
    "#                     # Save model checkpoint\n",
    "#                     output_dir = os.path.join(args['output_dir'], 'checkpoint-{}'.format(global_step))\n",
    "#                     if not os.path.exists(output_dir):\n",
    "#                         os.makedirs(output_dir)\n",
    "#                     model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "#                     model_to_save.save_pretrained(output_dir)\n",
    "#                     logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "#     return global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment this if you want to train for the first time\n",
    "# if args['do_train']:\n",
    "#     train_dataset = load_and_cache_examples(task, tokenizer, 'train')\n",
    "#     eval_dataset = load_and_cache_examples(task, tokenizer, 'eval')\n",
    "#     test_dataset = load_and_cache_examples(task, tokenizer, 'test')\n",
    "#     dataset = [train_dataset, eval_dataset, test_dataset]\n",
    "#     global_step= train(dataset, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment this if you want to train for the first time\n",
    "# if args['do_train']:\n",
    "#     if not os.path.exists(args['output_dir']):\n",
    "#             os.makedirs(args['output_dir'])\n",
    "#     logger.info(\"Saving model checkpoint to %s\", args['output_dir'])\n",
    "    \n",
    "#     model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "#     model_to_save.save_pretrained(args['output_dir'])\n",
    "#     tokenizer.save_pretrained(args['output_dir'])\n",
    "#     torch.save(args, os.path.join(args['output_dir'], 'training_args.bin'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples_seedly(task, tokenizer, mode):\n",
    "    processor = seedly_processor[task]()\n",
    "    output_mode = args['output_mode']\n",
    "    \n",
    "    \n",
    "    label_list = processor.get_labels()\n",
    "    if mode == 'train':\n",
    "        examples = processor.get_train_examples('data_seedly/')\n",
    "    elif mode == 'eval':\n",
    "        examples = processor.get_dev_examples('data_seedly/')\n",
    "    elif mode == 'test':\n",
    "        examples = processor.get_test_examples('data_seedly/')\n",
    "    \n",
    "        \n",
    "    if __name__ == \"__main__\":\n",
    "        dataset=yelpDataset(examples, label_list, args, tokenizer, output_mode)\n",
    "    return dataset\n",
    "\n",
    "def evaluate_yelp(model, tokenizer, prefix=\"\"):\n",
    "    EVAL_TASK = args['task_name']\n",
    "    test_dataset = load_and_cache_examples(task, tokenizer, 'test')\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args['eval_batch_size'])\n",
    "\n",
    "    # Eval!\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    for batch in tqdm_notebook(test_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
    "                      'labels':         batch[3]}\n",
    "            outputs = model(batch[0], batch[3])\n",
    "            _, logits = outputs[:2]\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    if args['output_mode'] == \"classification\":\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "    elif args['output_mode'] == \"regression\":\n",
    "        preds = np.squeeze(preds)\n",
    "    result, wrong = compute_metrics(EVAL_TASK, preds, out_label_ids)\n",
    "    confusion_matrix = result['cf']\n",
    "    df_cm = pd.DataFrame(confusion_matrix, index = [i for i in \"12345\"],\n",
    "                      columns = [i for i in \"12345\"])\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(df_cm, annot=True, cmap='gist_earth_r')\n",
    "    acc=(confusion_matrix[0,0] + confusion_matrix[1,1] + confusion_matrix[2,2] + confusion_matrix[3,3] + confusion_matrix[4,4])/np.sum(np.sum(confusion_matrix))\n",
    "    print(\"\\nTest Accuracy of LSTM model trained on yelp dataset tested on yelp: {:.4f}\".format(acc))\n",
    "    \n",
    "    total_param = 0\n",
    "    for param in model.parameters():\n",
    "        # print(param.data.size())\n",
    "        total_param += np.prod(list(param.data.size()))\n",
    "    print('\\nTotal parameters:', total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_yelp()\n",
    "model.to(device)\n",
    "checkpoint = args['output_dir']+WEIGHTS_NAME\n",
    "print('Loading model from {}'.format(checkpoint))\n",
    "state_dict = torch.load(checkpoint, map_location='cpu')\n",
    "model.load_state_dict(state_dict)\n",
    "# start seedly training,,,returns model\n",
    "evaluate_yelp(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_seedly(model, tokenizer, prefix=\"\"):\n",
    "    EVAL_TASK = args['task_name']\n",
    "    \n",
    "    test_dataset = load_and_cache_examples_seedly(task, tokenizer, 'test')\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args['eval_batch_size'])\n",
    "\n",
    "    # Eval!\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    for batch in tqdm_notebook(test_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
    "                      'labels':         batch[3]}\n",
    "            outputs = model(batch[0], batch[3])\n",
    "            _, logits = outputs[:2]\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    if args['output_mode'] == \"classification\":\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "    elif args['output_mode'] == \"regression\":\n",
    "        preds = np.squeeze(preds)\n",
    "    result, wrong = compute_metrics(EVAL_TASK, preds, out_label_ids)\n",
    "    confusion_matrix = result['cf']\n",
    "    df_cm = pd.DataFrame(confusion_matrix, index = [i for i in \"12345\"],\n",
    "                      columns = [i for i in \"12345\"])\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(df_cm, annot=True, cmap='gist_earth_r')\n",
    "    acc=(confusion_matrix[0,0] + confusion_matrix[1,1] + confusion_matrix[2,2] + confusion_matrix[3,3] + confusion_matrix[4,4])/np.sum(np.sum(confusion_matrix))\n",
    "    print(\"\\nTest Accuracy of LSTM model trained on yelp dataset tested on seedly: {:.4f}\".format(acc))\n",
    "    \n",
    "    total_param = 0\n",
    "    for param in model.parameters():\n",
    "        # print(param.data.size())\n",
    "        total_param += np.prod(list(param.data.size()))\n",
    "    print('\\nTotal parameters:', total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transfer(train_dataset, model, tokenizer):\n",
    "    \n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args['train_batch_size'])\n",
    "    \n",
    "    t_total = len(train_dataloader) // args['gradient_accumulation_steps'] * args['num_train_epochs']\n",
    "    \n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args['weight_decay']},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    \n",
    "    warmup_steps = math.ceil(t_total * args['warmup_ratio'])\n",
    "    args['warmup_steps'] = warmup_steps if args['warmup_steps'] == 0 else args['warmup_steps']\n",
    "    \n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'])\n",
    "    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args['warmup_steps'], t_total=t_total)\n",
    "    \n",
    "    if args['fp16']:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args['fp16_opt_level'])\n",
    "        \n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n",
    "    logger.info(\"  Total train batch size  = %d\", args['train_batch_size'])\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args['gradient_accumulation_steps'])\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    \n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")\n",
    "    epoch = 0\n",
    "    for _ in train_iterator:\n",
    "        epoch += 1\n",
    "        epoch_iterator = tqdm_notebook(train_dataloader, desc=\"Iteration\")\n",
    "        tr_loss, logging_loss = 0.0, 0.0\n",
    "        nb_train_steps = 0\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
    "                      'labels':         batch[3]}\n",
    "            outputs = model(batch[0], batch[3])\n",
    "            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "            print(\"\\r%f\" % loss, end='')\n",
    "            nb_train_steps += 1\n",
    "\n",
    "\n",
    "            if args['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "            if args['fp16']:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args['max_grad_norm'])\n",
    "\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "\n",
    "                if args['save_steps'] > 0 and global_step % args['save_steps'] == 0:\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args['output_dir'], 'checkpoint-{}'.format(global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "        \n",
    "\n",
    "    return global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Running training *****\n",
      "INFO:__main__:  Num examples = 4382\n",
      "INFO:__main__:  Num Epochs = 20\n",
      "INFO:__main__:  Total train batch size  = 64\n",
      "INFO:__main__:  Gradient Accumulation steps = 1\n",
      "INFO:__main__:  Total optimization steps = 1380\n",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from outputs_yelp_logged_LSTM_report/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6480373fac14a5fac0057823e41036e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.610788"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   5%|▌         | 1/20 [00:26<08:14, 26.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.930149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9121063f9940949127bc2683881f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.749606"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|█         | 2/20 [00:51<07:43, 25.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1.014582\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435053cf420549d981665b0a9932e1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.643544"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  15%|█▌        | 3/20 [01:16<07:16, 25.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.640107\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b047cf372c4aa08543ee0f9696e2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.888232"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 4/20 [01:42<06:53, 25.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.706704\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143186e64d0441538345fdda391a8cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.667993"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  25%|██▌       | 5/20 [02:08<06:27, 25.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.945572\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7c0f2b117f4084926911f72bd0fc56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.570999"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|███       | 6/20 [02:35<06:03, 25.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.749739\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875c359c85e647a6bcdd8d7f397dc236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.503128"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  35%|███▌      | 7/20 [03:02<05:43, 26.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.696324\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e4d6dc0c9845a6a8c9b3c2ebfcadae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.573101"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████      | 8/20 [03:29<05:20, 26.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.746265\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d154511f0274817ba46ca8726b16304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.678335"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  45%|████▌     | 9/20 [03:57<04:55, 26.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.317306\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49cbb22313ce4bb0b51fe11b5afbb0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.301968"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 10/20 [04:25<04:33, 27.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.304494\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c70328b9e53447c9235a439ce289084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.479243"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  55%|█████▌    | 11/20 [04:54<04:09, 27.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.623222\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c27717fd174ff6833012388fd4d3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.314631"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|██████    | 12/20 [05:22<03:42, 27.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.402144\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e19800173040dabc03b1c9d8e5f6f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.322716"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  65%|██████▌   | 13/20 [05:49<03:13, 27.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.232034\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be1f59dca1b40ce84e42e809ded5069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.238261"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|███████   | 14/20 [06:15<02:42, 27.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.556993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f36e427a4544a0fb1803a869179a077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.353752"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  75%|███████▌  | 15/20 [06:41<02:14, 26.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.410835\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9baea55d31ca4f11a130e149455567b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.383927"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|████████  | 16/20 [07:09<01:48, 27.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.319817\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b055342860459ebda382feac212b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.396302"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  85%|████████▌ | 17/20 [07:37<01:22, 27.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.206001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d7e6ef51554e00bd6b3a0691e76bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.298361"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|█████████ | 18/20 [08:05<00:55, 27.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.237503\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5365bc3e7514661837729a25d2440fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.341278"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  95%|█████████▌| 19/20 [08:31<00:27, 27.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.265791\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7481c9bf8af40a7a84efc668b1a05de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=69, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.283752"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 20/20 [08:57<00:00, 26.89s/it]\n",
      "INFO:__main__:Saving model checkpoint to outputs_seedly_logged_LSTM_report_transfer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.305967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Model weights saved in outputs_seedly_logged_LSTM_report_transfer/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "### Uncomment this if you want to train for the first time\n",
    "# model = LSTM_yelp()\n",
    "# model.to(device)\n",
    "# checkpoint = args['output_dir']+WEIGHTS_NAME\n",
    "# print('Loading model from {}'.format(checkpoint))\n",
    "# state_dict = torch.load(checkpoint, map_location='cpu')\n",
    "# model.load_state_dict(state_dict)\n",
    "# # start seedly training,,,returns model\n",
    "# args['output_dir']='outputs_seedly_logged_LSTM_report_transfer'\n",
    "# args['num_train_epochs'] = 20\n",
    "\n",
    "# if os.path.exists(args['output_dir']) and os.listdir(args['output_dir']) and args['do_train'] and not args['overwrite_output_dir']:\n",
    "#     raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args['output_dir']))\n",
    "\n",
    "# if args['do_train']:\n",
    "#     train_dataset = load_and_cache_examples_seedly(task, tokenizer, 'train')\n",
    "#     global_step= train_transfer(train_dataset, model, tokenizer)\n",
    "#     if not os.path.exists(args['output_dir']):\n",
    "#             os.makedirs(args['output_dir'])\n",
    "#     logger.info(\"Saving model checkpoint to %s\", args['output_dir'])\n",
    "    \n",
    "#     model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "#     model_to_save.save_pretrained(args['output_dir'])\n",
    "#     tokenizer.save_pretrained(args['output_dir'])\n",
    "#     torch.save(args, os.path.join(args['output_dir'], 'training_args.bin'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from outputs_seedly_logged_LSTM_report_transfer/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f2132a68fb40ea9ffa662a9978e00f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=9, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test Accuracy of LSTM model trained on yelp dataset tested on seedly: 0.6442\n",
      "\n",
      "Total parameters: 45896453\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "args['output_dir']='outputs_seedly_logged_LSTM_report_transfer/'\n",
    "model = LSTM_yelp()\n",
    "model.to(device)\n",
    "checkpoint = args['output_dir']+WEIGHTS_NAME\n",
    "print('Loading model from {}'.format(checkpoint))\n",
    "state_dict = torch.load(checkpoint, map_location='cpu')\n",
    "model.load_state_dict(state_dict)\n",
    "evaluate_seedly(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
