{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "import random\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm_notebook, trange\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "\n",
    "from pytorch_transformers import (WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer)\n",
    "\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "from utils_seedly import (convert_examples_to_features,\n",
    "                        output_modes, processors)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for training\n",
    "Configuration for such as: director for data, log and outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'data_dir': 'data_seedly/',\n",
    "    'model_type':  'bert',\n",
    "    'model_name': 'bert-base-cased',\n",
    "    'task_name': 'binary',\n",
    "    'output_dir': 'outputs_seedly_logged_BERT_report/',\n",
    "    'cache_dir': 'cache/',\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'fp16': False,\n",
    "    'fp16_opt_level': 'O1',\n",
    "    'max_seq_length': 128,\n",
    "    'output_mode': 'classification',\n",
    "    'train_batch_size': 64,\n",
    "    'eval_batch_size': 64,\n",
    "\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'num_train_epochs': 100,\n",
    "    'weight_decay': 0,\n",
    "    'learning_rate': 4e-5,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'warmup_ratio': 0.06,\n",
    "    'warmup_steps': 0,\n",
    "    'max_grad_norm': 1.0,\n",
    "\n",
    "    'logging_steps': 50,\n",
    "    'evaluate_during_training': False,\n",
    "    'save_steps': 2000,\n",
    "    'eval_all_checkpoints': True,\n",
    "\n",
    "    'overwrite_output_dir': False,\n",
    "    'reprocess_input_data': False,\n",
    "    'notes': 'Using Yelp Reviews dataset'\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment this if you want to train for the first time\n",
    "# if os.path.exists(args['output_dir']) and os.listdir(args['output_dir']) and args['do_train'] and not args['overwrite_output_dir']:\n",
    "#     raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args['output_dir']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_class, model_class, tokenizer_class = (BertConfig, BertForSequenceClassification, BertTokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer for BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/chungsoo002/.cache/torch/pytorch_transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6\n",
      "INFO:pytorch_transformers.modeling_utils:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"binary\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 5,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/chungsoo002/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    }
   ],
   "source": [
    "config = config_class.from_pretrained(args['model_name'], num_labels=5, finetuning_task=args['task_name'])\n",
    "tokenizer = tokenizer_class.from_pretrained(args['model_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained BERT model\n",
    "Also specify the specific details of the architecture: such as number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment this if you want to train for the first time\n",
    "# model = model_class.from_pretrained(args['model_name'], num_labels=5, hidden_dropout_prob=0.3)\n",
    "# model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "Here we define functions to efficiently load data and then feed into the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = args['task_name']\n",
    "\n",
    "if task in processors.keys() and task in output_modes.keys():\n",
    "    processor = processors[task]()\n",
    "    label_list = processor.get_labels()\n",
    "    num_labels = len(label_list)\n",
    "else:\n",
    "    raise KeyError(f'{task} not found in processors or in output_modes. Please check utils.py.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class yelpDataset(Dataset):\n",
    "  def __init__(self, examples, label_list, args, tokenizer, output_mode):\n",
    "    self.examples=examples\n",
    "    self.label_list = label_list\n",
    "    self.args = args\n",
    "    self.tokenizer = tokenizer\n",
    "    self.output_mode = output_mode\n",
    "      \n",
    "  def __len__(self):\n",
    "    # return size of dataset\n",
    "    return len(self.examples)\n",
    "      \n",
    "  def __getitem__(self, idx):\n",
    "    features = convert_examples_to_features(self.examples[idx], self.label_list, self.args['max_seq_length'], self.tokenizer, self.output_mode,\n",
    "                cls_token_at_end=bool(self.args['model_type'] in ['xlnet']),            \n",
    "                cls_token=self.tokenizer.cls_token,\n",
    "                cls_token_segment_id=2 if self.args['model_type'] in ['xlnet'] else 0,\n",
    "                sep_token=self.tokenizer.sep_token,\n",
    "                sep_token_extra=bool(self.args['model_type'] in ['roberta']),           \n",
    "                pad_on_left=bool(self.args['model_type'] in ['xlnet']),                 \n",
    "                pad_token=self.tokenizer.convert_tokens_to_ids([self.tokenizer.pad_token])[0],\n",
    "                pad_token_segment_id=4 if self.args['model_type'] in ['xlnet'] else 0)\n",
    "    \n",
    "    all_input_ids = torch.tensor(features.input_ids, dtype=torch.long)\n",
    "    all_input_mask = torch.tensor(features.input_mask, dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor(features.segment_ids, dtype=torch.long)\n",
    "    if self.output_mode == \"classification\":\n",
    "        all_label_ids = torch.tensor(features.label_id, dtype=torch.long)\n",
    "    elif self.output_mode == \"regression\":\n",
    "        all_label_ids = torch.tensor(features.label_id, dtype=torch.float)\n",
    "\n",
    "    dataset = (all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(task, tokenizer, mode):\n",
    "    processor = processors[task]()\n",
    "    output_mode = args['output_mode']\n",
    "    \n",
    "    \n",
    "    label_list = processor.get_labels()\n",
    "    if mode == 'train':\n",
    "        examples = processor.get_train_examples(args['data_dir'])\n",
    "    elif mode == 'eval':\n",
    "        examples = processor.get_dev_examples(args['data_dir'])\n",
    "    elif mode == 'test':\n",
    "        examples = processor.get_test_examples(args['data_dir'])\n",
    "    \n",
    "        \n",
    "    if __name__ == \"__main__\":\n",
    "        dataset=yelpDataset(examples, label_list, args, tokenizer, output_mode)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to compute evaluation metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, matthews_corrcoef, confusion_matrix\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def get_mismatched(labels, preds):\n",
    "    mismatched = labels != preds\n",
    "    examples = processor.get_dev_examples(args['data_dir'])\n",
    "    wrong = [i for (i, v) in zip(examples, mismatched) if v]\n",
    "    \n",
    "    return wrong\n",
    "\n",
    "def get_eval_report(labels, preds):\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    cf = confusion_matrix(labels, preds, labels=[0,1,2,3,4])\n",
    "    return {\n",
    "        \"mcc\": mcc,\n",
    "        \"cf\": cf\n",
    "    }, get_mismatched(labels, preds)\n",
    "\n",
    "def compute_metrics(task_name, preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "    return get_eval_report(labels, preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Eval, Test function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment this if you want to train for the first time\n",
    "# def train(dataset, model, tokenizer):\n",
    "#     EVAL_TASK = args['task_name']\n",
    "#     tb_writer = SummaryWriter('./log_seedly_train_BERT_report_lowdim')\n",
    "#     train_dataset, eval_dataset, test_dataset = dataset\n",
    "    \n",
    "#     train_sampler = RandomSampler(train_dataset)\n",
    "#     train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args['train_batch_size'])\n",
    "#     eval_sampler = SequentialSampler(eval_dataset)\n",
    "#     eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n",
    "#     test_sampler = SequentialSampler(test_dataset)\n",
    "#     test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args['eval_batch_size'])\n",
    "    \n",
    "#     t_total = len(train_dataloader) // args['gradient_accumulation_steps'] * args['num_train_epochs']\n",
    "    \n",
    "#     no_decay = ['bias', 'LayerNorm.weight']\n",
    "#     optimizer_grouped_parameters = [\n",
    "#         {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args['weight_decay']},\n",
    "#         {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "#         ]\n",
    "    \n",
    "#     warmup_steps = math.ceil(t_total * args['warmup_ratio'])\n",
    "#     args['warmup_steps'] = warmup_steps if args['warmup_steps'] == 0 else args['warmup_steps']\n",
    "    \n",
    "#     optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'])\n",
    "#     scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args['warmup_steps'], t_total=t_total)\n",
    "    \n",
    "#     if args['fp16']:\n",
    "#         try:\n",
    "#             from apex import amp\n",
    "#         except ImportError:\n",
    "#             raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "#         model, optimizer = amp.initialize(model, optimizer, opt_level=args['fp16_opt_level'])\n",
    "        \n",
    "#     logger.info(\"***** Running training *****\")\n",
    "#     logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "#     logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n",
    "#     logger.info(\"  Total train batch size  = %d\", args['train_batch_size'])\n",
    "#     logger.info(\"  Gradient Accumulation steps = %d\", args['gradient_accumulation_steps'])\n",
    "#     logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "#     global_step = 0\n",
    "    \n",
    "#     model.zero_grad()\n",
    "#     train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")\n",
    "#     epoch = 0\n",
    "#     for _ in train_iterator:\n",
    "#         epoch += 1\n",
    "#         epoch_iterator = tqdm_notebook(train_dataloader, desc=\"Iteration\")\n",
    "#         tr_loss, logging_loss = 0.0, 0.0\n",
    "#         tr_acc, log_acc = 0.0, 0.0\n",
    "#         nb_train_steps = 0\n",
    "#         for step, batch in enumerate(epoch_iterator):\n",
    "#             model.train()\n",
    "#             batch = tuple(t.to(device) for t in batch)\n",
    "#             inputs = {'input_ids':      batch[0],\n",
    "#                       'attention_mask': batch[1],\n",
    "#                       'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
    "#                       'labels':         batch[3]}\n",
    "#             outputs = model(**inputs)\n",
    "#             loss, logits = outputs[0:2]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "#             print(\"\\r%f\" % loss, end='')\n",
    "#             nb_train_steps += 1\n",
    "#             preds = logits.detach().cpu().numpy()\n",
    "#             out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "\n",
    "#             if args['output_mode'] == \"classification\":\n",
    "#                 preds = np.argmax(preds, axis=1)\n",
    "#             elif args['output_mode'] == \"regression\":\n",
    "#                 preds = np.squeeze(preds)\n",
    "#             result, wrong = compute_metrics(EVAL_TASK, preds, out_label_ids)\n",
    "#             c = result['cf']\n",
    "#             acc=(c[0,0] + c[1,1] + c[2,2] + c[3,3] + c[4,4])/np.sum(np.sum(c))\n",
    "\n",
    "\n",
    "#             if args['gradient_accumulation_steps'] > 1:\n",
    "#                 loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "#             if args['fp16']:\n",
    "#                 with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "#                     scaled_loss.backward()\n",
    "#                 torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args['max_grad_norm'])\n",
    "\n",
    "#             else:\n",
    "#                 loss.backward()\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n",
    "\n",
    "#             tr_loss += loss.item()\n",
    "#             tr_acc = (tr_acc * (nb_train_steps-1) + acc)/(nb_train_steps) \n",
    "#             if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "#                 optimizer.step()\n",
    "#                 scheduler.step()  # Update learning rate schedule\n",
    "#                 model.zero_grad()\n",
    "#                 global_step += 1\n",
    "\n",
    "# #                 if args['logging_steps'] > 0 and global_step % args['logging_steps'] == 0:\n",
    "# #                     # Log metrics\n",
    "# #                     if args['evaluate_during_training']:  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "# #                         results, _ = evaluate(model, tokenizer)\n",
    "# #                         for key, value in results.items():\n",
    "# #                             tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
    "# #                     tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "# #                     tb_writer.add_scalar('loss', (tr_loss - logging_loss)/args['logging_steps'], global_step)\n",
    "# #                     logging_loss = tr_loss\n",
    "\n",
    "#                 if args['save_steps'] > 0 and global_step % args['save_steps'] == 0:\n",
    "#                     # Save model checkpoint\n",
    "#                     output_dir = os.path.join(args['output_dir'], 'checkpoint-{}'.format(global_step))\n",
    "#                     if not os.path.exists(output_dir):\n",
    "#                         os.makedirs(output_dir)\n",
    "#                     model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "#                     model_to_save.save_pretrained(output_dir)\n",
    "#                     logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "#         tr_loss = tr_loss/nb_train_steps\n",
    "#         # Eval!\n",
    "#         prefix = global_step\n",
    "#         logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "#         logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "#         logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "#         eval_loss = 0.0\n",
    "#         nb_eval_steps = 0\n",
    "#         preds = None\n",
    "#         out_label_ids = None\n",
    "#         for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
    "#             model.eval()\n",
    "#             batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 inputs = {'input_ids':      batch[0],\n",
    "#                           'attention_mask': batch[1],\n",
    "#                           'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
    "#                           'labels':         batch[3]}\n",
    "#                 outputs = model(**inputs)\n",
    "#                 tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "#                 eval_loss += tmp_eval_loss.mean().item()\n",
    "#             nb_eval_steps += 1\n",
    "#             if preds is None:\n",
    "#                 preds = logits.detach().cpu().numpy()\n",
    "#                 out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "#             else:\n",
    "#                 preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "#                 out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "#         eval_loss = eval_loss / nb_eval_steps\n",
    "#         if args['output_mode'] == \"classification\":\n",
    "#             preds = np.argmax(preds, axis=1)\n",
    "#         elif args['output_mode'] == \"regression\":\n",
    "#             preds = np.squeeze(preds)\n",
    "#         result, wrong = compute_metrics(EVAL_TASK, preds, out_label_ids)\n",
    "#         c = result['cf']\n",
    "#         eval_acc=(c[0,0] + c[1,1] + c[2,2] + c[3,3] + c[4,4])/np.sum(np.sum(c))\n",
    "        \n",
    "#         # Test!\n",
    "#         logger.info(\"***** Running test {} *****\".format(prefix))\n",
    "#         logger.info(\"  Num examples = %d\", len(test_dataset))\n",
    "#         logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "#         test_loss = 0.0\n",
    "#         nb_test_steps = 0\n",
    "#         preds = None\n",
    "#         out_label_ids = None\n",
    "#         for batch in tqdm_notebook(test_dataloader, desc=\"Testing\"):\n",
    "#             model.eval()\n",
    "#             batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 inputs = {'input_ids':      batch[0],\n",
    "#                           'attention_mask': batch[1],\n",
    "#                           'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
    "#                           'labels':         batch[3]}\n",
    "#                 outputs = model(**inputs)\n",
    "#                 tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "#                 test_loss += tmp_eval_loss.mean().item()\n",
    "#             nb_test_steps += 1\n",
    "#             if preds is None:\n",
    "#                 preds = logits.detach().cpu().numpy()\n",
    "#                 out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "#             else:\n",
    "#                 preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "#                 out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "#         test_loss = test_loss / nb_test_steps\n",
    "#         if args['output_mode'] == \"classification\":\n",
    "#             preds = np.argmax(preds, axis=1)\n",
    "#         elif args['output_mode'] == \"regression\":\n",
    "#             preds = np.squeeze(preds)\n",
    "#         result, wrong = compute_metrics(EVAL_TASK, preds, out_label_ids)\n",
    "#         c = result['cf']\n",
    "#         test_acc=(c[0,0] + c[1,1] + c[2,2] + c[3,3] + c[4,4])/np.sum(np.sum(c))\n",
    "        \n",
    "#         # Log metrics\n",
    "#         tb_writer.add_scalar('train_loss', tr_loss, epoch)\n",
    "#         tb_writer.add_scalar('eval_loss', eval_loss, epoch)\n",
    "#         tb_writer.add_scalar('test_loss', test_loss, epoch)\n",
    "#         tb_writer.add_scalar('train_acc', tr_acc, epoch)\n",
    "#         tb_writer.add_scalar('eval_acc', eval_acc, epoch)\n",
    "#         tb_writer.add_scalar('test_acc', test_acc, epoch)\n",
    "\n",
    "#     return global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment this if you want to train for the first time\n",
    "# if args['do_train']:\n",
    "#     train_dataset = load_and_cache_examples(task, tokenizer, 'train')\n",
    "#     eval_dataset = load_and_cache_examples(task, tokenizer, 'eval')\n",
    "#     test_dataset = load_and_cache_examples(task, tokenizer, 'test')\n",
    "#     dataset = [train_dataset, eval_dataset, test_dataset]\n",
    "#     global_step= train(dataset, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment this if you want to train for the first time\n",
    "# if args['do_train']:\n",
    "#     if not os.path.exists(args['output_dir']):\n",
    "#             os.makedirs(args['output_dir'])\n",
    "#     logger.info(\"Saving model checkpoint to %s\", args['output_dir'])\n",
    "    \n",
    "#     model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "#     model_to_save.save_pretrained(args['output_dir'])\n",
    "#     tokenizer.save_pretrained(args['output_dir'])\n",
    "#     torch.save(args, os.path.join(args['output_dir'], 'training_args.bin'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, prefix=\"\"):\n",
    "    EVAL_TASK = args['task_name']\n",
    "    test_dataset = load_and_cache_examples(task, tokenizer, 'test')\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args['eval_batch_size'])\n",
    "\n",
    "    # Eval!\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    for batch in tqdm_notebook(test_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
    "                      'labels':         batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            _, logits = outputs[:2]\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    if args['output_mode'] == \"classification\":\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "    elif args['output_mode'] == \"regression\":\n",
    "        preds = np.squeeze(preds)\n",
    "    result, wrong = compute_metrics(EVAL_TASK, preds, out_label_ids)\n",
    "    confusion_matrix = result['cf']\n",
    "    df_cm = pd.DataFrame(confusion_matrix, index = [i for i in \"12345\"],\n",
    "                      columns = [i for i in \"12345\"])\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(df_cm, annot=True, cmap='gist_earth_r')\n",
    "    acc=(confusion_matrix[0,0] + confusion_matrix[1,1] + confusion_matrix[2,2] + confusion_matrix[3,3] + confusion_matrix[4,4])/np.sum(np.sum(confusion_matrix))\n",
    "    print(\"\\nTest Accuracy of BERT model trained on seedly dataset: {:.4f}\".format(acc))\n",
    "    \n",
    "    total_param = 0\n",
    "    for param in model.parameters():\n",
    "        # print(param.data.size())\n",
    "        total_param += np.prod(list(param.data.size()))\n",
    "    print('\\nTotal parameters:', total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.modeling_utils:loading configuration file outputs_seedly_logged_BERT_report/checkpoint-2000/config.json\n",
      "INFO:pytorch_transformers.modeling_utils:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.3,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 5,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "INFO:pytorch_transformers.modeling_utils:loading weights file outputs_seedly_logged_BERT_report/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f077774482cb4f23ae43d5312e9134e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=9, style=ProgressStyle(description_width='inâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test Accuracy of BERT model trained on seedly dataset: 0.6715\n",
      "\n",
      "Total parameters: 108314117\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model_class.from_pretrained('outputs_seedly_logged_BERT_report/checkpoint-2000')\n",
    "model.to(device);\n",
    "evaluate(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
